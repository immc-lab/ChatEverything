# Incontext Learning Papers

<img src="https://img.shields.io/badge/%F0%9F%A5%B3-Welcome-brightgreen">

-  [2023](#2023-back-to-top‚á™) | [2022](#2022-back-to-top‚á™) | [2021](#2021-back-to-top‚á™) 
-  [Back to HOMEüè†](../README.md)

## Papers
### 2023 [[Back to Top‚á™](#Incontext-Learning-Papers)]
| Year | Title                                                                                                                   | Venue      | Paper                                         | Code                                                                           |
|------|-------------------------------------------------------------------------------------------------------------------------|------------|-----------------------------------------------|--------------------------------------------------------------------------------|
| 2023 | Resources and Few-shot Learners for In-context Learning in Slavic Languages                                             | EACL 2023  | [link](https://arxiv.org/pdf/2304.01922.pdf)  | [link](https://github.com/fewshot-goes-multilingual/slavic-incontext-learning) |
| 2023 | Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback                            | Pre-print  | [link](https://arxiv.org/pdf/2305.10142.pdf)  | [link](https://github.com/FranxYao/GPT-Bargaining)                             |
| 2023 | Least-to-Most Prompting Enables Complex Reasoning in Large Language Models                                              | ICLR 2023  | [link](https://arxiv.org/pdf/2205.10625.pdf)  | -                                                                              |
| 2023 | Prompting GPT-3 To Be Reliable                                                                                          | ICLR 2023  | [link](https://arxiv.org/pdf/2210.09150.pdf)  | [link](https://github.com/NoviScl/GPT3-Reliability)                            |
| 2023 | Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering | ICLR 2023  | [link](https://arxiv.org/pdf/2212.10375.pdf)  | [link](https://github.com/Shark-NLP/self-adaptive-ICL)                         |
| 2023 | On the Relation between Sensitivity and Accuracy in In-Context Learning                                                 | arXiv 2023 | [link](https://arxiv.org/pdf/2209.07661.pdf)  | [link](https://github.com/yandachen/ICLSensitivity)                            |
| 2023 | Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers                    | ACL 2023   | [link](https://arxiv.org/pdf/2212.10559.pdf)  | [link](https://github.com/microsoft/LMOps)                                     |
| 2023 | Transformers as Algorithms: Generalization and Stability in In-context Learning                                         |            | [link](https://arxiv.org/pdf/2301.07067.pdf)  | [link](https://github.com/yingcong-li/transformers-as-algorithms)              |
| 2023 | Can In-context Learners Learn a Reasoning Concept from Demonstrations?                                                  |            | [link](https://arxiv.org/pdf/2212.01692.pdf)  |                                                                                |
| 2023 | The Flan Collection: Designing Data and Methods for Effective Instruction Tuning                                        |            | [link](https://arxiv.org/pdf/2301.13688.pdf)  | [link](https://github.com/google-research/FLAN/tree/main/flan/v2)              |
|      |                                                                                                                         |            |                                               |                                                                                |
|      |                                                                                                                         |            |                                               |                                                                                |
|      |                                                                                                                         |            |                                               |                                                                                |
|      |                                                                                                                         |            |                                               |                                                                                |
|      |                                                                                                                         |            |                                               |                                                                                |
### 2022 [[Back to Top‚á™](#Incontext-Learning-Papers)]

| Year | Title                                                                                                                                                     | Venue          | Paper                                                  | Code                                                                      |
|------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|--------------------------------------------------------|---------------------------------------------------------------------------|
| 2022 | What learning algorithm is in-context learning? Investigations with linear models                                                                         |                |[link](https://arxiv.org/pdf/2211.15661.pdf)|[link](https://github.com/ekinakyurek/google-research/tree/master/incontext)|
| 2022 | A Survey for In-context Learning                                                                                                                          | arXiv 2022     |[link](https://arxiv.org/pdf/2301.00234.pdf)|                                                                           |
| 2022 | MetaICL: Learning to Learn In Context NAACL 2022 a pretrained language model is tuned to do in-context learning on a large set of training tasks          | NACCL 2022     |[link](https://arxiv.org/pdf/2110.15943.pdf)|[link](https://github.com/dongguanting/In-Context-Learning_PaperList/tree/master)|
| 2022 | Improving In-Context Few-Shot Learning via Self-Supervised Training                                                                                       | NACCL 2022     |[link](https://aclanthology.org/2022.naacl-main.260.pdf)|[link](https://github.com/dongguanting/In-Context-Learning_PaperList/tree/master)|
| 2022 | Calibrate Before Use: Improving Few-shot Performance of Language Models.                                                                                  | ICML 2021      |[link](http://proceedings.mlr.press/v139/zhao21c.html)|[link](https://img.shields.io/badge/additional_calibration_parameters-D8D0E1)|
| 2022 | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models                                                                                     | NeurIPS 2022   |[link](https://arxiv.org/abs/2201.11903)|                                                                           |
| 2022 | Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator                                               | NACCL 2022     |[link](https://arxiv.org/pdf/2206.08082.pdf)|                                                                           |
| 2022 | Iteratively Prompt Pre-trained Language Models for Chain of Though                                                                                        | EMNLP 2022     |[link](https://arxiv.org/pdf/2203.08383.pdf)|[link](https://github.com/sunlab-osu/IterPrompt)|
| 2022 | Automatic Chain of Thought Prompting in Large Language Models                                                                                             | arXiv 2022     |[link](https://arxiv.org/abs/2210.03493)|[link](https://github.com/amazon-science/auto-cot)|
| 2022 | Learning To Retrieve Prompts for In-Context Learning                                                                                                      | NAACL-HLT 2022 |[link](https://arxiv.org/pdf/2112.08633.pdf)|[link](https://github.com/OhadRubin/EPR)|
| 2022 | Finetuned Language Models Are Zero-Shot Learners instruction tuning.                                                                                      | ICLR 2022      |[link](https://arxiv.org/abs/2109.01652)|[link](https://github.com/google-research/flan)|
| 2022 | Active Example Selection for In-Context Learning.                                                                                                         | EMNLP 2022     |[link](https://arxiv.org/pdf/2211.04486.pdf)|[link](https://github.com/chicagohai/active-example-selection)|
| 2022 | An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels                                                                       | ACL 2022       |[link](https://aclanthology.org/2022.acl-long.60.pdf)|[link](https://github.com/BYU-PCCL/information-theoretic-prompts)|
| 2022 | Demystifying Prompts in Language Models via Perplexity Estimation                                                                                         | arXiv 2022     |[link](https://arxiv.org/pdf/2212.04037.pdf)|[link](https://github.com/bigscience-workshop/promptsource)|
| 2022 | Structured Prompting:Scaling In-Context Learning to 1,000 Examples                                                                                        | arXiv 2022     |[link](https://arxiv.org/pdf/2212.06713.pdf)|[link](https://github.com/microsoft/LMOps)|
| 2022 | Fantastically Ordered Prompts and Where to Find Them:Overcoming Few-Shot Prompt Order Sensitivity                                                         | ACL 2022       |[link](https://arxiv.org/pdf/2104.08786.pdf)|                                                                           |
| 2022 | Can language models learn from explanations in context?                                                                                                   | arXiv 2022     |[link](https://arxiv.org/pdf/2204.02329.pdf)|                                                                           |
| 2022 | Prototypical Calibration for Few-shot Learning of Language Models                                                                                         | arXiv 2022     |[link](https://arxiv.org/pdf/2205.10183.pdf)|[link](https://github.com/microsoft/unilm)|
| 2022 | Cross-Task Generalization via Natural Language Crowdsourcing Instructions                                                                                 | ACL 2022       |[link](https://arxiv.org/pdf/2104.08773.pdf)|[link](https://github.com/allenai/natural-instructions-v1)|
| 2022 | Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?                                                                               | EMNLP 2022     |[link](https://arxiv.org/pdf/2202.12837.pdf)|[link](https://github.com/Alrope123/rethinking-demonstrations)|
| 2022 | Emergent Abilities of Large Language Models                                                                                                               | TMLR 2022      |[link](https://arxiv.org/pdf/2206.07682.pdf)|[link](https://github.com/inverse-scaling/prize)|
| 2022 | Ground-Truth Labels Matter:A Deeper Look into Input-Label Demonstrations                                                                                  | EMNLP 2022     |[link](https://arxiv.org/pdf/2205.12685.pdf)|                                                                           |
| 2022 | On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model                                                               | NAACL 2022     |[link](https://arxiv.org/pdf/2204.13509.pdf)|                                                                           |
| 2022 | Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale                                            | arXiv 2022     |[link](https://arxiv.org/pdf/2212.09095.pdf)|[link](https://github.com/amazon-science/llm-interpret)|
| 2022 | Data Distributional Properties Drive Emergent In-Context Learning in Transformers                                                                         | NeurIPS 2022   |[link](https://arxiv.org/pdf/2205.05055.pdf)|[link](https://github.com/deepmind/emergent_in_context_learning)|
| 2022 | Diverse Demonstrations Improve In-context Compositional Generalization                                                                                    | arXiv 2022     |[link](https://arxiv.org/pdf/2212.06800.pdf)|[link](https://github.com/itayle/diverse-demonstrations)|
| 2022 | Towards Understanding Chain-of-Thought Prompting:An Empirical Study of What Matters                                                                       | arXiv 2022     |[link](https://arxiv.org/pdf/2212.10001.pdf)|[link](https://github.com/sunlab-osu/Understanding-CoT)|
| 2022 | An Explanation of In-context Learning as Implicit Bayesian Inference                                                                                      | ICLR 2022      |[link](https://arxiv.org/pdf/2111.02080.pdf)|[link](https://github.com/p-lambda/incontext-learning)|
| 2022 | In-context Learning and Induction Heads                                                                                                                   | arXiv 2022     |[link](https://arxiv.org/ftp/arxiv/papers/2209/2209.11895.pdf)|                                                                           |
| 2022 | What Can Transformers Learn In-Context? A Case Study of Simple Function Classes                                                                           | NeurIPS 2022   |[link](https://openreview.net/pdf?id=flNZJ2eOet)|[link](https://github.com/dtsip/in-context-learning)|
| 2022 | Data Distributional Properties Drive Emergent In-Context Learning in Transformers                                                                         | NeurIPS 2022   |[link](https://arxiv.org/pdf/2205.05055.pdf)|[link](https://github.com/deepmind/emergent_in_context_learning)|
| 2022 | What learning algorithm is in-context learning? Investigations with linear models                                                                         | arXiv 2022     |[link](https://arxiv.org/pdf/2211.15661.pdf)|[link](https://github.com/ekinakyurek/google-research/tree/master/incontext)|
| 2022 | Transformers learn in-context by gradient descent                                                                                                         | arXiv 2022     |[link](https://arxiv.org/pdf/2212.07677.pdf)|                                                                           |
| 2022 | Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models                                                              | arXiv 2022     |[link](https://arxiv.org/pdf/2206.04615.pdf)|[link](https://github.com/google/BIG-bench)|
| 2022 | SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Task.                                                                 | EMNLP 2022     |[link](https://arxiv.org/pdf/2204.07705.pdf)|[link](https://github.com/allenai/natural-instructions)|
| 2022 | Language Models are Multilingual Chain-of-Thought Reasoners.                                                                                              | arXiv 2022     |[link](https://arxiv.org/pdf/2210.03057.pdf)|[link](https://github.com/google-research/url-nlp)|
| 2022 | Instruction Induction: From Few Examples to Natural Language Task Descriptions                                                                            | arXiv 2022     |[link](https://arxiv.org/pdf/2205.10782.pdf)|[link](https://github.com/orhonovich/instruction-induction)|
| 2022 | Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor.                                                                              | arXiv 2022     |[link](https://arxiv.org/pdf/2212.09689.pdf)|[link](https://github.com/orhonovich/unnatural-instructions)|
| 2022 | SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions                                                                                   |                |[link](https://arxiv.org/pdf/2212.10560.pdf)|[link](https://github.com/yizhongw/self-instruct)|
| 2022 | Meta-learning via Language Model In-context Tuning                                                                                                        | ACL 2022       |[link](https://arxiv.org/pdf/2110.07814.pdf)|[link](https://github.com/yandachen/In-context-Tuning)|
| 2022 | Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation. | COLING 2022    |[link](https://aclanthology.org/2022.coling-1.56.pdf)|[link](https://github.com/passing2961/EmpGPT-3)|
| 2022 | In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models.                                                  |                |[link](https://arxiv.org/pdf/2212.10670.pdf)|                                                                           |
| 2022 | The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design                                                                          | ICLR 2022      |[link](https://arxiv.org/pdf/2110.04541.pdf)|                                                                           |

### 2021 [[Back to Top‚á™](#Incontext-Learning-Papers)]

| Year | Title                                           | Venue      | Paper                                 | Code                                                                    |
|------|-------------------------------------------------|------------|---------------------------------------|-------------------------------------------------------------------------|
| 2021 | What Makes Good In-Context Examples for GPT-3?  | arXiv 2021 |[link](https://arxiv.org/pdf/2101.06804.pdf)|[link](https://github.com/google-research/language/tree/master/language/totto)|
|      |       |       |       |      |
|      |       |       |       |      |
|      |       |       |       |      |
|      |       |       |       |      |
|      |       |       |       |      |
|      |       |       |       |      |
|      |       |       |       |      |
## Contributors

<p align="center"><a href="https://github.com/yangqqq-yq"><img src="https://avatars.githubusercontent.com/u/64053857?v=4" width="50px" alt="0xmatchmaker" /></a>&nbsp;&nbsp;<a href="https://github.com/LeeRoc-China"><img src="https://avatars.githubusercontent.com/u/59104898?s=400&u=c225a082a6a410e3d7c84ca29a07d723d7308dca&v=4" width="50px" alt="0xmatchmaker" /></a>&nbsp;&nbsp;</p>

>>>>>>> 1503e706f6595fa233a0d7f405cde6c7dce81fea
