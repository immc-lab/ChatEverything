# Huge Model Paper

<img src="https://img.shields.io/badge/%F0%9F%A5%B3-Welcome-brightgreen">

-  [NLP](#Huge Models for NLP) | [CV](#Huge Models for CV) | [Multimodal](#Huge Models for Multimodal) 
-  [Back to HOMEüè†](../README.md)

## Papers

### Huge Models for NLP [[Back to Top‚á™](#Huge Models for NLP)]
| Year | Model Name | Title | Venue                                        | Paper                                                                                                                           | Code                                                                         |
|------| -----| ---------------------------------------------------------------------------|----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| 2023 | Dolly 2.0 | - | -                                            | -                                                                                                                               | [link](https://github.com/zhengzangw/awesome-huge-models/tree/main)          |
| 2023 | StableLM | - | - | -| [link](https://huggingface.co/stabilityai)                                   |
| 2023 | Cerabras-GPT | Training Compute-Optimal Large Language Models                     | arxiv                                        | [link](https://arxiv.org/abs/2203.15556)                                                                                        | -                                                                            |
| 2023 | LLaMa | Open and Efficient Foundation Language Models                             | arxiv                                        | [link](https://arxiv.org/pdf/2302.13971v1.pdf)                                                                                  | [link](https://github.com/facebookresearch/llama)                            |
| 2022 | BLOOM  | A 176B-Parameter Open-Access Multilingual Language Model                 | arxiv                                        | [link](https://arxiv.org/pdf/2211.05100.pdf)                                                                                    | [link](https://huggingface.co/bigscience/bloom)                              |                                                                                                                            |
| 2022 | Galactica| A scientific language model trained on over 48 million scientific texts | arxiv                                        | [linl](https://arxiv.org/pdf/2211.09085.pdf)                                                                                    | [link](https://huggingface.co/facebook/galactica-1.3b)                       |                                                                                                                                     |
| 2022 | GLM-130B | GLM-130B: An Open Bilingual Pre-trained Model | ICLR 2023                                    | [link](https://arxiv.org/pdf/2210.02414.pdf)                                                                                    | [link](https://github.com/THUDM/GLM-130B)                                    |
| 2022 | UL2 | Unifying Language Learning Paradigms | arxiv                                        | [link](https://arxiv.org/pdf/2205.05131.pdf)                                                                                    | [link](https://huggingface.co/google/ul2)                                    |                                                                                                                                        |
| 2022 | OPT | OPT: Open Pre-trained Transformer Language Models | arxiv                                        | [link](https://arxiv.org/pdf/2205.01068.pdf)                                                                                    | [link](https://github.com/facebookresearch/metaseq)                          |                                                                                                                                  |
| 2022 | PaLM | PaLM: Scaling Language Modeling with Pathways | arxiv                                        | [link](https://arxiv.org/pdf/2204.02311.pdf)                                                                                    | -                                                                            |            |                                                                                                                                   |
| 2022 | GPT-NeoX | GPT-NeoX-20B: An Open-Source Autoregressive Language Model | ACL 2022                                     | [link](https://arxiv.org/pdf/2204.06745.pdf)                                                                                    | [link](https://github.com/EleutherAI/gpt-neox)                               |                                                                                                                                            |
| 2022 | InstructGPT | Training language models to follow instructions with human feedback | [link](https://arxiv.org/pdf/2203.02155.pdf) | -                                                                                                                               |                                                                              |                                                                                                                                   |
| 2022 | EVA 2.0 | EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Trainingraining language mode | arxiv                                        | [link](https://arxiv.org/pdf/2203.09313.pdf)                                                                                    | [link](https://openi.pcl.ac.cn/BAAI/WuDao-Model/src/branch/master)           |            |                                                                                                                                   |
| 2022 | AlphaCode | Competition-Level Code Generation with AlphaCode | arxiv                                        | [link](https://arxiv.org/pdf/2203.07814.pdf)                                                                                    | -                                                                            |
| 2022 | ST-MoE | ST-MoE: Designing Stable and Transferable Sparse Expert Models | arxiv                                        | [link](https://arxiv.org/pdf/2202.08906.pdf)                                                                                    | -                                                                            |
| 2022 | LaMDA | LaMDA: Language Models for Dialog Applications | arxiv                                        | [link](https://arxiv.org/pdf/2201.08239.pdf)                                                                                    | -                                                                            |     
| 2022 | ERNIE-ViLG | ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation | arxiv                                        | [link](https://arxiv.org/pdf/2112.15283.pdf)                                                                                    | -                                                                            |  
| 2021 | GLaM | GLaM: Efficient Scaling of Language Models with Mixture-of-Experts | ICML 2022                                    | [link](https://arxiv.org/pdf/2112.06905.pdf)                                                                                    | -                                                                            |  
| 2021 | Gopher | Scaling Language Models: Methods, Analysis & Insights from Training Gopher | arxiv                                        | [link](https://arxiv.org/pdf/2112.11446.pdf)                                                                                    | -                                                                            |  
| 2021 | Yuan 1.0 | Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning | arxiv                                        | [link](https://arxiv.org/pdf/2110.04725.pdf)                                                                                    | -                                                                            |  
| 2021 | MT-NLG | Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model | arxiv                                        | [link](https://arxiv.org/pdf/2201.11990.pdf)                                                                                    | -                                                                            |  
| 2021 | Codex | Evaluating Large Language Models Trained on Code | arxiv                                        | [link](https://arxiv.org/pdf/2107.03374.pdf)                                                                                    | -                                                                            |  
| 2021 | ERNIE 3.0 | ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | arxiv                                        | [link](https://arxiv.org/pdf/2107.02137.pdf)                                                                                    | -                                                                            |  
| 2021 | HyperClova | What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers | EMNLP 2021                                   | [link](https://arxiv.org/pdf/2109.04650v1.pdf)                                                                                  | -                                                                            |  
| 2021 | ByT5 | ByT5: Towards a token-free future with pre-trained byte-to-byte models | TACL 2022                                    | [link](https://arxiv.org/pdf/2105.13626.pdf)                                                                                    | [link](https://github.com/google-research/byt5)                              |  
| 2021 | PanGu-Œ± | PanGu-Œ±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation | arxiv                                        | [link](https://arxiv.org/pdf/2104.12369.pdf)                                                                                    | -                                                                            |  
| 2021 | mT5 | mT5: A massively multilingual pre-trained text-to-text transformer | arxiv                                        | [link](https://arxiv.org/pdf/2010.11934.pdf)                                                                                    | [link](https://github.com/google-research/multilingual-t5)                   |  
| 2021 | GLM | GLM: General Language Model Pretraining with Autoregressive Blank Infilling | ACL 2022                                     | [link](https://arxiv.org/pdf/2103.10360.pdf)                                                                                    | [link](https://openi.pcl.ac.cn/BAAI/WuDao-Model/src/branch/master/GLM)       |  
| 2021 | Switch Transformer | Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | JMLR 2021                                    | [link](https://arxiv.org/pdf/2101.03961.pdf)                                                                                    | -                                                                            |                                                                      |  
| 2020 | CPM | CPM: A Large-scale Generative Chinese Pre-trained Language Model | arxiv                                        | [link](https://arxiv.org/pdf/2012.00413.pdf)                                                                                    | [link](https://github.com/TsinghuaAI/CPM)                                    |  
| 2020 | GPT-3 | Language Models are Few-Shot Learners | arxiv                                        | [link](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)                                       | -                                                                            |
| 2020 | Blender | Recipes for building an open-domain chatbot | arxiv                                        | [link](https://arxiv.org/pdf/2004.13637.pdf)                                                                                    | -                                                                            |
| 2020 | Meena | Towards a Human-like Open-Domain Chatbot | arxiv                                        | [link](https://arxiv.org/pdf/2001.09977.pdf)                                                                                    | -                                                                            |
| 2019 | DialoGPT | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | ACL 2020                                     | [link](https://arxiv.org/pdf/1911.00536.pdf)                                                                                    | [link](https://github.com/microsoft/DialoGPT)                                |
| 2019 | T5 | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | JMLR 2019                                    | [link](https://arxiv.org/pdf/1910.10683.pdf)                                                                                    | [link](https://github.com/google-research/text-to-text-transfer-transformer) |
| 2019 | Megatron-LM | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | arxiv                                        | [link](https://arxiv.org/pdf/1909.08053.pdf)                                                                                    | [link](https://github.com/NVIDIA/Megatron-LM)                                |
| 2019 | RoBERTa | RoBERTa: A Robustly Optimized BERT Pretraining Approach | arxiv                                        | [link](https://arxiv.org/pdf/1907.11692.pdf)                                                                                    | [link](https://github.com/facebookresearch/fairseq)                          |
| 2019 | XLNet | XLNet:Generalized Autoregressive Pretraining for Language Understanding | NeurIPS 2019                                 | [link](https://papers.nips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html)                                   | [link](https://github.com/zihangdai/xlnet)                                   |
| 2019 | GPT-2 | Language Models are Unsupervised Multitask Learners | arxiv                                        | [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)                   | [link](https://github.com/NVIDIA/Megatron-LM)                                |
| 2018 | GPT  | Improving Language Understanding by Generative Pre-Training                | -                                            | [link](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | [link](https://github.com/google-research/vmoe)                              |
| 2018 | BERT | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | NAACL 2018                                   | [link](https://arxiv.org/pdf/1810.04805.pdf)                                                                                    | [link](https://github.com/google-research/bert)                              |


### Huge Models for CV [[Back to Top‚á™](#Huge Models for CV)]

| Year  |  Model Name | Title                                                                      | Venue | Paper                                                                                                                                                                                                                                                                  | Code                                                                            |
|-------|------------|----------------------------------------------------------------------------|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
 | 2023  |  DINOv2| DINOv2: Learning Robust Visual Features without Supervision                | arxiv | [link](https://arxiv.org/pdf/2304.07193.pdf)                                                                                                                                                                                                                           | -                                                                               |
 | 2023  | SEEM | Segment Everything Everywhere All at Once                                  | arxiv | [link](https://arxiv.org/pdf/2304.06718.pdf)                                                                                                                                                                                                                           | [link](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) |
| 2023  | Visual ChatGPT | Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models | arxiv | [link](https://arxiv.org/pdf/2303.04671.pdf)                                                                                                                                                                                                                           | [link](https://github.com/microsoft/TaskMatrix)                                 |
| 2023  | PICASSO | -                                                                          | -     | -                                                                                                                                                                                                                                                                      | [official website](https://www.nvidia.com/zh-tw/gpu-cloud/picasso/)             |\
| 2023  | SAM | Segment Anything | -     | [link](https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=DsUiPPmfKCcAX9z0vCN&_nc_ht=scontent-lax3-2.xx&oh=00_AfCJOMTowYIhhAmJ9CBVsh-pg2LgSwqscB-g_KJAY_bWVQ&oe=646E5A27) | [official website](https://segment-anything.com/)                               |
| 2023  | ‰π¶Áîü | - | - | [official news](https://www.sensetime.com/cn/news-detail/51166318?categoryId=72)                                                                                                                                                                                        | -                                                                               |
| 2022  | AllInOne | All in One: Exploring Unified Video-Language Pre-training | arxiv | [link](https://arxiv.org/pdf/2203.07303.pdf)                                                                                                                                                                                                                           | [link](https://github.com/showlab/all-in-one)                                   |
| 2022  | CoCa | CoCa: Contrastive Captioners are Image-Text Foundation Models | arxiv | [link](https://arxiv.org/pdf/2205.01917.pdf)                                                                                                                                                                                                                           | - |
| 2012 | CoAtNet | CoAtNet: Marrying Convolution and Attention for All Data Sizes | arxiv | [link](https://arxiv.org/pdf/2106.04803.pdf)                                                                                                                                                                                                                           | - |
| 2021 | Swin Transformer | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | arxiv | [link](https://arxiv.org/pdf/2103.14030.pdf)                                                                                                                                                                                                                           | [link](https://github.com/microsoft/Swin-Transformer) |
| 2021 | V-MOE | Scaling Vision with Sparse Mixture of Experts | arxiv | [link](https://arxiv.org/pdf/2106.05974.pdf)                                                                                                                                                                                                                           | - |
| 2021 | ViT | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | ICLR2021 | [link](https://arxiv.org/pdf/2010.11929.pdf)                                                                                                                                                                                                                           | [link](https://github.com/google-research/vision_transformer) |

### Huge Models for Multimodal [[](#Huge Models for Multimodal)] 

| Year | Model Name | Title                                                                                                                                        | Venue | Paper                                                            | Code |
| ---- |-----------|----------------------------------------------------------------------------------------------------------------------------------------------| ----- |------------------------------------------------------------------|---|
| 2022 | DALL¬∑E2   | Hierarchical Text-Conditional Image Generation with CLIP Latents                                                                             | -                                         | [link](https://cdn.openai.com/papers/dall-e-2.pdf)               | - |
| 2022 | Imagen    | Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding                                                               | arxiv                                                            | [link](https://arxiv.org/pdf/2205.11487.pdf)               | - |
| 2022 |  LayoutLM v3         | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking                                                                 | arxiv                                                                          | [link](https://arxiv.org/pdf/2204.08387.pdf)               | - |
| 2022 |   VPT        | Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos                                                                 | arxiv                                                                          | [link ](https://arxiv.org/pdf/2206.11795.pdf)              | - |
| 2022 |   Gato        | A Generalist Agent                                                                                                                           | Transactions on Machine Learning Research                                      | [link](https://openreview.net/pdf?id=1ikK0kHjvj)               | - |
| 2022 | Â≠üÂ≠ê        | -                                                                                                                                            | arxiv                                                                                                                                   | [official website](https://arxiv.org/pdf/2105.13290.pdf) | - |
| 2021 |   Á¥´‰∏úÂ§™Âàù        | CogView: Mastering Text-to-Image Generation via Transformers                                                                                 | arxiv                                                                          | [link](https://cdn.openai.com/papers/dall-e-2.pdf)                                                                 | - |
| 2021 |    ÁõòÂè§       | PanGu-Œ±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation                                        | arxiv                                                                          | [link](https://arxiv.org/pdf/2104.12369.pdf)                     | - |
| 2021-2022 |  Chinese CLIP         | Learning Transferable Visual Models From Natural Language Supervision <br/> Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese | arxiv                                                                                                 | [link](https://arxiv.org/pdf/2103.00020.pdf)<br/>[link](https://arxiv.org/pdf/2211.01335.pdf)        | - |
## Contributors

<p align="center"><a href="https://github.com/Shyless111"><img src="https://avatars.githubusercontent.com/u/131270623?v=4" width="50px" alt="0xmatchmaker" /></a>&nbsp;&nbsp;<a href="https://github.com/zhangziyi1670"><img src="https://avatars.githubusercontent.com/u/75550266?v=4" width="50px" alt="0xmatchmaker" /></a>&nbsp;&nbsp;</p>

