# Summary of LLM fine-tuning

## LLM-Adapters
Adapters for LLMs refer to neural modules integrated into LLMs, which contain a small number of extra trainable parameters, allowing for eﬃcient ﬁne-tuning of speciﬁc tasks without aﬀecting
the pre-trained parameters of the LLM. Here we use the notation Θ to represent the parameters of the LLM and Φ to represent the parameters of the adapter module. During training, the parameters
of the LLM (Θ) remain ﬁxed, while the parameters of the adapter module (Φ) are adjusted to perform a speciﬁc task. As a result, the representations generated by the LLM are not distorted
due to task-speciﬁc tuning, while the adapter module acquires the capability to encode task-speciﬁc information.
### Parameter-Efficient-Tuning (PEFT)
| Adapter       | Title                                                                                          | Paper                                              | Code                                                             |
|---------------|------------------------------------------------------------------------------------------------|----------------------------------------------------|------------------------------------------------------------------|
| LoRA          | LORA:LOW-RANK ADAPTATION OF LARGE LANGUAGE MODEL                                               | [Link](https://arxiv.org/pdf/2106.09685.pdf)       | [Link](https://github.com/microsoft/LoRA)                        |
| AdapterH      | Parameter-Efficient Transfer Learning for NLP                                                  | [Link](https://arxiv.org/pdf/1902.00751.pdf)       | [Link](https://github.com/google-research/adapter-bert)          |
| AdapterP      | MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer                        | [Link](https://arxiv.org/pdf/2005.00052.pdf)       | [Link](https://adapterhub.ml/)                                   |
| Parallel      | TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING                                | [Link](https://arxiv.org/pdf/2110.04366.pdf)       | [Link](https://github.com/jxhe/unify-parameter-efficient-tuning) |
| Prefix Tuning | Prefix-Tuning: Optimizing Continuous Prompts for Generation                                    | [Link](https://aclanthology.org/2021.acl-long.353/) | [Link](https://github.com/XiangLi1999/PrefixTuning)              |
| P-Tuning      | GPT Understands, Too                                                                           | [Link](https://arxiv.org/pdf/2103.10385.pdf)       | [Link](https://github.com/THUDM/P-tuning)                        |
| P-Tuning v2   | P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Task | [Link](https://arxiv.org/pdf/2110.07602.pdf)       | [Link](https://github.com/THUDM/P-tuning-v2)                     |
| Prompt Tuning | The Power of Scale for Parameter-Efficient Prompt Tuning                                       | [Link](https://arxiv.org/pdf/2104.08691.pdf)       | -                                                                |

## Adapter-tuning
## Side-tuning
## Mask-tuning
## Prefix-tuning