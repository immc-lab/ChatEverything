# RLHF Related Papers

<img src="https://img.shields.io/badge/%F0%9F%A5%B3-Welcome-brightgreen">

-  [2023](#2023-back-to-top‚á™) | [2022](#2022-back-to-top‚á™) | [2021](#2021-back-to-top‚á™) 
-  [Back to HOMEüè†](https://github.com/immc-lab/FollowGPT)

## Papers

### 2023 [[Back to Top‚á™](#RLHF-Related-Papers)]

| Year | Title                                                        | Venue | Paper                                    | Code                                            |
| ---- | ------------------------------------------------------------ | ----- | ---------------------------------------- | ----------------------------------------------- |
| 2023 | GPT-4 Technical Report                                       | -     | [link](https://arxiv.org/abs/2303.08774) | -                                               |
| 2023 | RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment | -     | [link](https://arxiv.org/abs/2304.06767) | -                                               |
| 2023 | RRHF: Rank Responses to Align Language Models with Human Feedback without tears | -     | [link](https://arxiv.org/abs/2304.05302) | [link](https://github.com/GanjinZero/RRHF)      |
| 2023 | Better Aligning Text-to-Image Models with Human Preference   | -     | [link](https://arxiv.org/abs/2303.14420) | [link](https://tgxs002.github.io/align_sd_web/) |
| 2023 | ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation | -     | [link](https://arxiv.org/abs/2304.05977) | [link](https://github.com/THUDM/ImageReward)    |
| 2023 | Aligning Text-to-Image Models using Human Feedback           | -     | [link](https://arxiv.org/abs/2302.12192) | -                                               |
| 2023 | Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models | -     | [link](https://arxiv.org/abs/2303.04671) | [link](https://github.com/microsoft/TaskMatrix) |
| 2023 | Pretraining Language Models with Human Preferences           | -     | [link](https://arxiv.org/abs/2302.08582) | -                                               |
| 2023 | Aligning Language Models with Preferences through f-divergence Minimization | -     | [link](https://arxiv.org/abs/2302.08215) | -                                               |
| 2023 | Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons | -     | [link](https://arxiv.org/abs/2301.11270) | -                                               |
| 2023 | The Capacity for Moral Self-Correction in Large Language Models |       | [link](https://arxiv.org/abs/2302.07459) | -                                               |
|      |                                                              |       |                                          |                                                 |

### 2022 [[Back to Top‚á™](#RLHF-Related-Papers)]

| Year | Title                                                        | Venue | Paper                                    | Code                                                         |
| ---- | ------------------------------------------------------------ | ----- | ---------------------------------------- | ------------------------------------------------------------ |
| 2022 | Few-shot Preference Learning for Human-in-the-Loop RL        | -     | [link](https://arxiv.org/abs/2212.03363) | [link](https://sites.google.com/view/few-shot-preference-rl/home) |
| 2022 | Improving alignment of dialogue agents via targeted human judgements | -     | [link](https://arxiv.org/abs/2209.14375) | -                                                            |
| 2022 | Scaling Laws for Reward Model Overoptimization               | -     | [link](https://arxiv.org/abs/2210.10760) | -                                                            |
| 2022 | Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned | -     | [link](https://arxiv.org/abs/2209.07858) | -                                                            |
| 2022 | Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning | -     | [link](https://arxiv.org/abs/2208.02294) | -                                                            |
| 2022 | Quark: Controllable Text Generation with Reinforced Unlearning | -     | [link](https://arxiv.org/abs/2205.13636) | -                                                            |
| 2022 | Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback | -     | [link](https://arxiv.org/abs/2204.05862) | [link](https://github.com/anthropics/hh-rlhf)                |
| 2022 | Training language models to follow instructions with human feedback | -     | [link](https://arxiv.org/abs/2203.02155) | -                                                            |
| 2022 | Discovering Language Model Behaviors with Model-Written Evaluations | -     | [link](https://arxiv.org/abs/2212.09251) | [link](https://github.com/anthropics/evals)                  |
|      |                                                              |       |                                          |                                                              |
### 2021 [[Back to Top‚á™](#RLHF-Related-Papers)]

| Year | Title                                                        | Venue        | Paper                                     | Code |
| ---- | ------------------------------------------------------------ | ------------ | ----------------------------------------- | ---- |
| 2021 | Recursively Summarizing Books with Human Feedback            | -            | [link]( https://arxiv.org/abs/2109.10862) | -    |
| 2021 | Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation | -            | [link](https://arxiv.org/abs/2106.08942)  | -    |
| 2020 | Learning to summarize from human feedback                    | NeurIPS 2020 | [link](https://arxiv.org/abs/2009.01325)  | -    |
|      |                                                              |              |                                           |      |

## Contributors

<p align="center"><a href="https://github.com/LeeRoc-China"><img src="https://avatars.githubusercontent.com/u/59104898?s=400&u=c225a082a6a410e3d7c84ca29a07d723d7308dca&v=4" width="50px" alt="0xmatchmaker" /></a>&nbsp;&nbsp;<a href="https://github.com/yangqqq-yq"><img src="https://avatars.githubusercontent.com/u/64053857?v=4" width="50px" alt="0xmatchmaker" /></a>&nbsp;&nbsp;</p>
